{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from keyrover.datasets import *\n",
    "from keyrover.vision import *\n",
    "from keyrover import *\n",
    "\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_paths, _, valid_paths = split_train_test_valid(image_paths, 1, 0.1)\n",
    "\n",
    "SIZE = (256, 256)\n",
    "\n",
    "train_dataset = KeyboardCameraTransformDataset(train_paths, size=SIZE)\n",
    "valid_dataset = KeyboardCameraTransformDataset(valid_paths, size=SIZE)\n",
    "\n",
    "len(train_dataset), len(valid_dataset)"
   ],
   "id": "fa1ed208dcbfb545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset.set_augmentations([\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.GaussianNoise(sigma=0.01),\n",
    "    transforms.RandomApply([transforms.GaussianNoise(sigma=0.01)], p=0.5),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize(SIZE),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "valid_dataset.set_augmentations([\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean, std),\n",
    "])"
   ],
   "id": "272d3f6b579fc9fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img, target = train_dataset.random_img()\n",
    "print(\"Target:\", target)\n",
    "print(\"Image:\", img.min(), img.max())\n",
    "\n",
    "mean = torch.tensor([0.29174, 8.5515e-06, 0.023512, -0.20853, -0.80377, 3.3909], device=device)\n",
    "std = torch.tensor([0.14669, 0.047459, 1.1898, 0.9208, 0.71566, 0.85268], device=device)\n",
    "\n",
    "target = target.unsqueeze(0).to(device) * std + mean\n",
    "texcoords = prediction_to_texture_coordinates(target)\n",
    "imshow(img, texcoords[0])"
   ],
   "id": "6a0aa3a42aa9233e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dl_kwargs = {\"batch_size\": BATCH_SIZE, \"num_workers\": 2, \"persistent_workers\": True, \"pin_memory\": False}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **dl_kwargs, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, **dl_kwargs)"
   ],
   "id": "3aaaa590762f072b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "class CornersRegressionModel(pl.LightningModule):\n",
    "    def __init__(self, lr: float | None = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze the parameters of the pre-trained layers\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the parameters of the last few layers for fine-tuning\n",
    "        # for param in self.model.layer4.parameters():\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        self.model.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.model.fc.in_features, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 6),\n",
    "        )\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.lr = self.learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def predict(self, image: torch.Tensor) -> np.ndarray:\n",
    "        image = image.to(self.device)\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = self.forward(image)\n",
    "        pred = prediction_to_texture_coordinates(pred)\n",
    "\n",
    "        if len(pred) == 1:\n",
    "            return pred[0,]\n",
    "        return pred\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(image)\n",
    "\n",
    "    def _step(self, batch: tuple[torch.Tensor, torch.Tensor], stage: str) -> float:\n",
    "        image, target = batch\n",
    "        predictions = self.model(image)\n",
    "\n",
    "        loss = self.loss_fn(predictions, target)\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> float:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> float:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> float:\n",
    "        return self._step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self) -> dict:\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ],
   "id": "c803d05afc0212ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb.login()"
   ],
   "id": "5088164a695d5ab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "wandb.finish()\n",
    "model = CornersRegressionModel(lr=LEARNING_RATE)\n",
    "model"
   ],
   "id": "b909910e23f0a3b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summarize(model)",
   "id": "7af3e9f29a52790f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logger = WandbLogger(project=\"mrover-keyboard-corner-prediction\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(log_every_n_steps=1, logger=logger, max_epochs=100, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)"
   ],
   "id": "fcfd884d3cc7700d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), f\"models/transform-prediction/{wandb.run.name}.pt\")",
   "id": "481e07cba1509119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = CornersRegressionModel()\n",
    "model.load_state_dict(torch.load(f\"models/transform-prediction/balmy-sponge-6.pt\", weights_only=True))"
   ],
   "id": "d1983c975772f58c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "image, target = valid_dataset.random_img()\n",
    "image = image.to(device).unsqueeze(0)\n",
    "target = target.to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():   \n",
    "    pred = model(image)\n",
    "\n",
    "pred = (pred * std + mean)\n",
    "target = (target * std + mean)\n",
    "\n",
    "pred = prediction_to_texture_coordinates(pred)\n",
    "target = prediction_to_texture_coordinates(target)\n",
    "\n",
    "show_images([image[0], pred[0], image[0], target[0]])"
   ],
   "id": "e791838382143da5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vidcap = cv2.VideoCapture(f\"{TEST_DATASET}/110.mp4\")\n",
    "total = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "frame = 150\n",
    "vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "_, image = vidcap.read()\n",
    "\n",
    "image = test_transforms(image).unsqueeze(0)\n",
    "image = image.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(image)\n",
    "\n",
    "pred = (pred * std + mean)\n",
    "pred = prediction_to_texture_coordinates(pred)\n",
    "\n",
    "imshow(image[0], pred[0])"
   ],
   "id": "6e5d87b7e0d8a206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(image.min(), image.max())\n",
    "image.shape"
   ],
   "id": "8925dc7361c4729d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
